<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="markdown" source-language="en-US" target-language="pl-pl">
    <header>
      <tool tool-id="mdxliff" tool-name="mdxliff" tool-version="1.0-04506c2" tool-company="Microsoft" />
      <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">921ed8aae918795262436fe0afcf61d923de4573</xliffext:olfilehash>
      <xliffext:olfilepath xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">docs\parallel\openmp\d-using-the-schedule-clause.md</xliffext:olfilepath>
      <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">ht</xliffext:oltranslationpriority>
      <xliffext:oltranslationtype xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">Human Translation</xliffext:oltranslationtype>
      <xliffext:olskeletonhash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">1cbe37f99ea8f57549f14f5b0ff22b50c984bda1</xliffext:olskeletonhash>
      <xliffext:olxliffhash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">bb4e1e404d4fff14d15a9d371b81ca84edb92ddf</xliffext:olxliffhash>
    </header>
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>D.</source>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Using the schedule Clause | Microsoft Docs</source>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>D.</source>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Using the schedule Clause</source>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>A parallel region has at least one barrier, at its end, and may have additional barriers within it.</source>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>At each barrier, the other members of the team must wait for the last thread to arrive.</source>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>To minimize this wait time, shared work should be distributed so that all threads arrive at the barrier at about the same time.</source>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>If some of that shared work is contained in <bpt id="p1">**</bpt>for<ept id="p1">**</ept> constructs, the <ph id="ph1">`schedule`</ph> clause can be used for this purpose.</source>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>When there are repeated references to the same objects, the choice of schedule for a <bpt id="p1">**</bpt>for<ept id="p1">**</ept> construct may be determined primarily by characteristics of the memory system, such as the presence and size of caches and whether memory access times are uniform or nonuniform.</source>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>Such considerations may make it preferable to have each thread consistently refer to the same set of elements of an array in a series of loops, even if some threads are assigned relatively less work in some of the loops.</source>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>This can be done by using the <bpt id="p1">**</bpt>static<ept id="p1">**</ept> schedule with the same bounds for all the loops.</source>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>In the following example, note that zero is used as the lower bound in the second loop, even though <bpt id="p1">**</bpt>k<ept id="p1">**</ept> would be more natural if the schedule were not important.</source>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>In the remaining examples, it is assumed that memory access is not the dominant consideration, and, unless otherwise stated, that all threads receive comparable computational resources.</source>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>In these cases, the choice of schedule for a <bpt id="p1">**</bpt>for<ept id="p1">**</ept> construct depends on all the shared work that is to be performed between the nearest preceding barrier and either the implied closing barrier or the nearest subsequent barrier, if there is a <ph id="ph1">`nowait`</ph> clause.</source>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>For each kind of schedule, a short example shows how that schedule kind is likely to be the best choice.</source>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>A brief discussion follows each example.</source>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>static<ept id="p1">**</ept> schedule is also appropriate for the simplest case, a parallel region containing a single <bpt id="p2">**</bpt>for<ept id="p2">**</ept> construct, with each iteration requiring the same amount of work.</source>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>static<ept id="p1">**</ept> schedule is characterized by the properties that each thread gets approximately the same number of iterations as any other thread, and each thread can independently determine the iterations assigned to it.</source>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>Thus no synchronization is required to distribute the work, and, under the assumption that each iteration requires the same amount of work, all threads should finish at about the same time.</source>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>For a team of <ph id="ph1">`p`</ph> threads, let <bpt id="p1">*</bpt>ceiling(n/p)<ept id="p1">*</ept> be the integer <bpt id="p2">*</bpt>q<ept id="p2">*</ept>, which satisfies <bpt id="p3">*</bpt>n = p<ph id="ph2">\*</ph>q - r<ept id="p3">*</ept> with <bpt id="p4">*</bpt>0 &lt;= r &lt; p<ept id="p4">*</ept>.</source>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>One implementation of the <bpt id="p1">**</bpt>static<ept id="p1">**</ept> schedule for this example would assign <bpt id="p2">*</bpt>q<ept id="p2">*</ept> iterations to the first <bpt id="p3">*</bpt>pâ€“1<ept id="p3">*</ept> threads, and <bpt id="p4">*</bpt>q-r<ept id="p4">*</ept> iterations to the last thread.</source>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Another acceptable implementation would assign <bpt id="p1">*</bpt>q<ept id="p1">*</ept> iterations to the first <bpt id="p2">*</bpt>p-r<ept id="p2">*</ept> threads, and <bpt id="p3">*</bpt>q-1<ept id="p3">*</ept> iterations to the remaining <bpt id="p4">*</bpt>r<ept id="p4">*</ept> threads.</source>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>This illustrates why a program should not rely on the details of a particular implementation.</source>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>dynamic<ept id="p1">**</ept> schedule is appropriate for the case of a <bpt id="p2">**</bpt>for<ept id="p2">**</ept> construct with the iterations requiring varying, or even unpredictable, amounts of work.</source>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>dynamic<ept id="p1">**</ept> schedule is characterized by the property that no thread waits at the barrier for longer than it takes another thread to execute its final iteration.</source>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>This requires that iterations be assigned one at a time to threads as they become available, with synchronization for each assignment.</source>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>The synchronization overhead can be reduced by specifying a minimum chunk size <bpt id="p1">*</bpt>k<ept id="p1">*</ept> greater than 1, so that threads are assigned <bpt id="p2">*</bpt>k<ept id="p2">*</ept> at a time until fewer than <bpt id="p3">*</bpt>k<ept id="p3">*</ept> remain.</source>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>This guarantees that no thread waits at the barrier longer than it takes another thread to execute its final chunk of (at most) <bpt id="p1">*</bpt>k<ept id="p1">*</ept> iterations.</source>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>dynamic<ept id="p1">**</ept> schedule can be useful if the threads receive varying computational resources, which has much the same effect as varying amounts of work for each iteration.</source>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>Similarly, the dynamic schedule can also be useful if the threads arrive at the <bpt id="p1">**</bpt>for<ept id="p1">**</ept> construct at varying times, though in some of these cases the <bpt id="p2">**</bpt>guided<ept id="p2">**</ept> schedule may be preferable.</source>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>guided<ept id="p1">**</ept> schedule is appropriate for the case in which the threads may arrive at varying times at a <bpt id="p2">**</bpt>for<ept id="p2">**</ept> construct with each iteration requiring about the same amount of work.</source>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>This can happen if, for example, the <bpt id="p1">**</bpt>for<ept id="p1">**</ept> construct is preceded by one or more sections or <bpt id="p2">**</bpt>for<ept id="p2">**</ept> constructs with <ph id="ph1">`nowait`</ph> clauses.</source>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Like <bpt id="p1">**</bpt>dynamic<ept id="p1">**</ept>, the <bpt id="p2">**</bpt>guided<ept id="p2">**</ept> schedule guarantees that no thread waits at the barrier longer than it takes another thread to execute its final iteration, or final <bpt id="p3">*</bpt>k<ept id="p3">*</ept> iterations if a chunk size of <bpt id="p4">*</bpt>k<ept id="p4">*</ept> is specified.</source>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>Among such schedules, the <bpt id="p1">**</bpt>guided<ept id="p1">**</ept> schedule is characterized by the property that it requires the fewest synchronizations.</source>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>For chunk size <bpt id="p1">*</bpt>k<ept id="p1">*</ept>, a typical implementation will assign <bpt id="p2">*</bpt>q = ceiling(n/p)<ept id="p2">*</ept> iterations to the first available thread, set <bpt id="p3">*</bpt>n<ept id="p3">*</ept> to the larger of <bpt id="p4">*</bpt>n-q<ept id="p4">*</ept> and <bpt id="p5">*</bpt>p<ph id="ph1">\*</ph>k<ept id="p5">*</ept>, and repeat until all iterations are assigned.</source>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>When the choice of the optimum schedule is not as clear as it is for these examples, the <bpt id="p1">**</bpt>runtime<ept id="p1">**</ept> schedule is convenient for experimenting with different schedules and chunk sizes without having to modify and recompile the program.</source>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>It can also be useful when the optimum schedule depends (in some predictable way) on the input data to which the program is applied.</source>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>To see an example of the trade-offs between different schedules, consider sharing 1000 iterations among 8 threads.</source>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>Suppose there is an invariant amount of work in each iteration, and use that as the unit of time.</source>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>If all threads start at the same time, the <bpt id="p1">**</bpt>static<ept id="p1">**</ept> schedule will cause the construct to execute in 125 units, with no synchronization.</source>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>But suppose that one thread is 100 units late in arriving.</source>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>Then the remaining seven threads wait for 100 units at the barrier, and the execution time for the whole construct increases to 225.</source>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>Because both the <bpt id="p1">**</bpt>dynamic<ept id="p1">**</ept> and <bpt id="p2">**</bpt>guided<ept id="p2">**</ept> schedules ensure that no thread waits for more than one unit at the barrier, the delayed thread causes their execution times for the construct to increase only to 138 units, possibly increased by delays from synchronization.</source>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>If such delays are not negligible, it becomes important that the number of synchronizations is 1000 for <bpt id="p1">**</bpt>dynamic<ept id="p1">**</ept> but only 41 for <bpt id="p2">**</bpt>guided<ept id="p2">**</ept>, assuming the default chunk size of one.</source>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>With a chunk size of 25, <bpt id="p1">**</bpt>dynamic<ept id="p1">**</ept> and <bpt id="p2">**</bpt>guided<ept id="p2">**</ept> both finish in 150 units, plus any delays from the required synchronizations, which now number only 40 and 20, respectively.</source>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>