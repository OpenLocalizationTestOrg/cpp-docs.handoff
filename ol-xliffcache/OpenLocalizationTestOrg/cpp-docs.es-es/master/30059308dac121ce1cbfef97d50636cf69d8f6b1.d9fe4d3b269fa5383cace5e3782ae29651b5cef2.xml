{"nodes":[{"pos":[12,40],"content":"Using Tiles | Microsoft Docs","needQuote":false,"needEscape":true,"nodes":[{"content":"Using Tiles | Microsoft Docs","pos":[0,28]}]},{"content":"Using Tiles","pos":[521,532]},{"content":"You can use tiling to maximize the acceleration of your app.","pos":[533,593]},{"content":"Tiling divides threads into equal rectangular subsets or <bpt id=\"p1\">*</bpt>tiles<ept id=\"p1\">*</ept>.","pos":[594,659],"source":" Tiling divides threads into equal rectangular subsets or *tiles*."},{"content":"If you use an appropriate tile size and tiled algorithm, you can get even more acceleration from your C++ AMP code.","pos":[660,775]},{"content":"The basic components of tiling are:","pos":[776,811]},{"content":"variables.","pos":[833,843]},{"content":"The primary benefit of tiling is the performance gain from <ph id=\"ph1\">`tile_static`</ph> access.","pos":[844,924],"source":" The primary benefit of tiling is the performance gain from `tile_static` access."},{"content":"Access to data in <ph id=\"ph1\">`tile_static`</ph> memory can be significantly faster than access to data in the global space (<ph id=\"ph2\">`array`</ph> or <ph id=\"ph3\">`array_view`</ph> objects).","pos":[925,1066],"source":" Access to data in `tile_static` memory can be significantly faster than access to data in the global space (`array` or `array_view` objects)."},{"content":"An instance of a <ph id=\"ph1\">`tile_static`</ph> variable is created for each tile, and all threads in the tile have access to the variable.","pos":[1067,1189],"source":" An instance of a `tile_static` variable is created for each tile, and all threads in the tile have access to the variable."},{"content":"In a typical tiled algorithm, data is copied into <ph id=\"ph1\">`tile_static`</ph> memory once from global memory and then accessed many times from the <ph id=\"ph2\">`tile_static`</ph> memory.","pos":[1190,1344],"source":" In a typical tiled algorithm, data is copied into `tile_static` memory once from global memory and then accessed many times from the `tile_static` memory."},{"content":"<bpt id=\"p1\">[</bpt>tile_barrier::wait Method<ept id=\"p1\">](reference/tile-barrier-class.md#wait)</ept>.","pos":[1352,1418],"source":"[tile_barrier::wait Method](reference/tile-barrier-class.md#wait)."},{"content":"A call to <ph id=\"ph1\">`tile_barrier::wait`</ph> suspends execution of the current thread until all of the threads in the same tile reach the call to <ph id=\"ph2\">`tile_barrier::wait`</ph>.","pos":[1419,1572],"source":" A call to `tile_barrier::wait` suspends execution of the current thread until all of the threads in the same tile reach the call to `tile_barrier::wait`."},{"content":"You cannot guarantee the order that the threads will run in, only that no threads in the tile will execute past the call to <ph id=\"ph1\">`tile_barrier::wait`</ph> until all of the threads have reached the call.","pos":[1573,1765],"source":" You cannot guarantee the order that the threads will run in, only that no threads in the tile will execute past the call to `tile_barrier::wait` until all of the threads have reached the call."},{"content":"This means that by using the <ph id=\"ph1\">`tile_barrier::wait`</ph> method, you can perform tasks on a tile-by-tile basis rather than a thread-by-thread basis.","pos":[1766,1907],"source":" This means that by using the `tile_barrier::wait` method, you can perform tasks on a tile-by-tile basis rather than a thread-by-thread basis."},{"content":"A typical tiling algorithm has code to initialize the <ph id=\"ph1\">`tile_static`</ph> memory for the whole tile followed by a call to <ph id=\"ph2\">`tile_barrer::wait`</ph>.","pos":[1908,2044],"source":" A typical tiling algorithm has code to initialize the `tile_static` memory for the whole tile followed by a call to `tile_barrer::wait`."},{"content":"Code that follows <ph id=\"ph1\">`tile_barrier::wait`</ph> contains computations that require access to all the <ph id=\"ph2\">`tile_static`</ph> values.","pos":[2045,2158],"source":" Code that follows `tile_barrier::wait` contains computations that require access to all the `tile_static` values."},{"content":"Local and global indexing.","pos":[2169,2195]},{"content":"You have access to the index of the thread relative to the entire <ph id=\"ph1\">`array_view`</ph> or <ph id=\"ph2\">`array`</ph> object and the index relative to the tile.","pos":[2196,2328],"source":" You have access to the index of the thread relative to the entire `array_view` or `array` object and the index relative to the tile."},{"content":"Using the local index can make your code easier to read and debug.","pos":[2329,2395]},{"content":"Typically, you use local indexing to access <ph id=\"ph1\">`tile_static`</ph> variables, and global indexing to access <ph id=\"ph2\">`array`</ph> and <ph id=\"ph3\">`array_view`</ph> variables.","pos":[2396,2530],"source":" Typically, you use local indexing to access `tile_static` variables, and global indexing to access `array` and `array_view` variables."},{"content":"<bpt id=\"p1\">[</bpt>tiled_extent Class<ept id=\"p1\">](../../parallel/amp/reference/tiled-extent-class.md)</ept> and <bpt id=\"p2\">[</bpt>tiled_index Class<ept id=\"p2\">](../../parallel/amp/reference/tiled-index-class.md)</ept>.","pos":[2538,2686],"source":"[tiled_extent Class](../../parallel/amp/reference/tiled-extent-class.md) and [tiled_index Class](../../parallel/amp/reference/tiled-index-class.md)."},{"content":"You use a <ph id=\"ph1\">`tiled_extent`</ph> object instead of an <ph id=\"ph2\">`extent`</ph> object in the <ph id=\"ph3\">`parallel_for_each`</ph> call.","pos":[2687,2781],"source":" You use a `tiled_extent` object instead of an `extent` object in the `parallel_for_each` call."},{"content":"You use a <ph id=\"ph1\">`tiled_index`</ph> object instead of an <ph id=\"ph2\">`index`</ph> object in the <ph id=\"ph3\">`parallel_for_each`</ph> call.","pos":[2782,2874],"source":" You use a `tiled_index` object instead of an `index` object in the `parallel_for_each` call."},{"pos":[2881,3045],"content":"To take advantage of tiling, your algorithm must partition the compute domain into tiles and then copy the tile data into <ph id=\"ph1\">`tile_static`</ph> variables for faster access.","source":"To take advantage of tiling, your algorithm must partition the compute domain into tiles and then copy the tile data into `tile_static` variables for faster access."},{"content":"Example of Global, Tile, and Local Indices","pos":[3054,3096]},{"content":"The following diagram represents an 8x9 matrix of data that is arranged in 2x3 tiles.","pos":[3100,3185]},{"content":"8&amp;#45;by&amp;#45;9 matrix divided into 2&amp;#45;by&amp;#45;3 tiles","pos":[3194,3249],"source":"8&#45;by&#45;9 matrix divided into 2&#45;by&#45;3 tiles"},{"content":"The following example displays the global, tile, and local indices of this tiled matrix.","pos":[3323,3411]},{"content":"An <ph id=\"ph1\">`array_view`</ph> object is created by using elements of type <ph id=\"ph2\">`Description`</ph>.","pos":[3412,3486],"source":" An `array_view` object is created by using elements of type `Description`."},{"content":"The <ph id=\"ph1\">`Description`</ph> holds the global, tile, and local indices of the element in the matrix.","pos":[3487,3576],"source":" The `Description` holds the global, tile, and local indices of the element in the matrix."},{"content":"The code in the call to <ph id=\"ph1\">`parallel_for_each`</ph> sets the values of the global, tile, and local indices of each element.","pos":[3577,3692],"source":" The code in the call to `parallel_for_each` sets the values of the global, tile, and local indices of each element."},{"content":"The output displays the values in the <ph id=\"ph1\">`Description`</ph> structures.","pos":[3693,3756],"source":" The output displays the values in the `Description` structures."},{"pos":[7895,8008],"content":"The main work of the example is in the definition of the <ph id=\"ph1\">`array_view`</ph> object and the call to <ph id=\"ph2\">`parallel_for_each`</ph>.","source":"The main work of the example is in the definition of the `array_view` object and the call to `parallel_for_each`."},{"pos":[8018,8099],"content":"The vector of <ph id=\"ph1\">`Description`</ph> structures is copied into an 8x9 <ph id=\"ph2\">`array_view`</ph> object.","source":"The vector of `Description` structures is copied into an 8x9 `array_view` object."},{"content":"The <ph id=\"ph1\">`parallel_for_each`</ph> method is called with a <ph id=\"ph2\">`tiled_extent`</ph> object as the compute domain.","pos":[8109,8201],"source":"The `parallel_for_each` method is called with a `tiled_extent` object as the compute domain."},{"content":"The <ph id=\"ph1\">`tiled_extent`</ph> object is created by calling the <ph id=\"ph2\">`extent::tile()`</ph> method of the <ph id=\"ph3\">`descriptions`</ph> variable.","pos":[8202,8309],"source":" The `tiled_extent` object is created by calling the `extent::tile()` method of the `descriptions` variable."},{"content":"The type parameters of the call to <ph id=\"ph1\">`extent::tile()`</ph>, <ph id=\"ph2\">`&lt;2,3&gt;`</ph>, specify that 2x3 tiles are created.","pos":[8310,8407],"source":" The type parameters of the call to `extent::tile()`, `<2,3>`, specify that 2x3 tiles are created."},{"content":"Thus, the 8x9 matrix is tiled into 12 tiles, four rows and three columns.","pos":[8408,8481]},{"content":"The <ph id=\"ph1\">`parallel_for_each`</ph> method is called by using a <ph id=\"ph2\">`tiled_index&lt;2,3&gt;`</ph> object (<ph id=\"ph3\">`t_idx`</ph>) as the index.","pos":[8491,8592],"source":"The `parallel_for_each` method is called by using a `tiled_index<2,3>` object (`t_idx`) as the index."},{"content":"The type parameters of the index (<ph id=\"ph1\">`t_idx`</ph>) must match the type parameters of the compute domain (<ph id=\"ph2\">`descriptions.extent.tile&lt; 2, 3&gt;()`</ph>).","pos":[8593,8727],"source":" The type parameters of the index (`t_idx`) must match the type parameters of the compute domain (`descriptions.extent.tile< 2, 3>()`)."},{"pos":[8737,8949],"content":"When each thread is executed, the index <ph id=\"ph1\">`t_idx`</ph> returns information about which tile the thread is in (<ph id=\"ph2\">`tiled_index::tile`</ph> property) and the location of the thread within the tile (<ph id=\"ph3\">`tiled_index::local`</ph> property).","source":"When each thread is executed, the index `t_idx` returns information about which tile the thread is in (`tiled_index::tile` property) and the location of the thread within the tile (`tiled_index::local` property)."},{"content":"Tile Synchronization—tile_static and tile_barrier::wait","pos":[8958,9013]},{"content":"The previous example illustrates the tile layout and indices, but is not in itself very useful.","pos":[9017,9112]},{"content":"Tiling becomes useful when the tiles are integral to the algorithm and exploit <ph id=\"ph1\">`tile_static`</ph> variables.","pos":[9114,9217],"source":"  Tiling becomes useful when the tiles are integral to the algorithm and exploit `tile_static` variables."},{"content":"Because all threads in a tile have access to <ph id=\"ph1\">`tile_static`</ph> variables, calls to <ph id=\"ph2\">`tile_barrier::wait`</ph> are used to synchronize access to the <ph id=\"ph3\">`tile_static`</ph> variables.","pos":[9218,9380],"source":" Because all threads in a tile have access to `tile_static` variables, calls to `tile_barrier::wait` are used to synchronize access to the `tile_static` variables."},{"content":"Although all of the threads in a tile have access to the <ph id=\"ph1\">`tile_static`</ph> variables, there is no guaranteed order of execution of threads in the tile.","pos":[9381,9528],"source":" Although all of the threads in a tile have access to the `tile_static` variables, there is no guaranteed order of execution of threads in the tile."},{"content":"The following example shows how to use <ph id=\"ph1\">`tile_static`</ph> variables and the <ph id=\"ph2\">`tile_barrier::wait`</ph> method to calculate the average value of each tile.","pos":[9529,9672],"source":" The following example shows how to use `tile_static` variables and the `tile_barrier::wait` method to calculate the average value of each tile."},{"content":"Here are the keys to understanding the example:","pos":[9673,9720]},{"content":"The rawData is stored in an 8x8 matrix.","pos":[9730,9769]},{"content":"The tile size is 2x2.","pos":[9779,9800]},{"content":"This creates a 4x4 grid of tiles and the averages can be stored in a 4x4 matrix by using an <ph id=\"ph1\">`array`</ph> object.","pos":[9801,9908],"source":" This creates a 4x4 grid of tiles and the averages can be stored in a 4x4 matrix by using an `array` object."},{"content":"There are only a limited number of types that you can capture by reference in an AMP-restricted function.","pos":[9909,10014]},{"content":"The <ph id=\"ph1\">`array`</ph> class is one of them.","pos":[10015,10048],"source":" The `array` class is one of them."},{"content":"The matrix size and sample size are defined by using <ph id=\"ph1\">`#define`</ph> statements, because the type parameters to <ph id=\"ph2\">`array`</ph>, <ph id=\"ph3\">`array_view`</ph>, <ph id=\"ph4\">`extent`</ph>, and <ph id=\"ph5\">`tiled_index`</ph> must be constant values.","pos":[10058,10239],"source":"The matrix size and sample size are defined by using `#define` statements, because the type parameters to `array`, `array_view`, `extent`, and `tiled_index` must be constant values."},{"content":"You can also use <ph id=\"ph1\">`const int static`</ph> declarations.","pos":[10240,10289],"source":" You can also use `const int static` declarations."},{"content":"As an additional benefit, it is trivial to change the sample size to calculate the average over 4x4 tiles.","pos":[10290,10396]},{"content":"A <ph id=\"ph1\">`tile_static`</ph> 2x2 array of float values is declared for each tile.","pos":[10406,10474],"source":"A `tile_static` 2x2 array of float values is declared for each tile."},{"content":"Although the declaration is in the code path for every thread, only one array is created for each tile in the matrix.","pos":[10475,10592]},{"content":"There is a line of code to copy the values in each tile to the <ph id=\"ph1\">`tile_static`</ph> array.","pos":[10602,10685],"source":"There is a line of code to copy the values in each tile to the `tile_static` array."},{"content":"For each thread, after the value is copied to the array, execution on the thread stops due to the call to <ph id=\"ph1\">`tile_barrier::wait`</ph>.","pos":[10686,10813],"source":" For each thread, after the value is copied to the array, execution on the thread stops due to the call to `tile_barrier::wait`."},{"content":"When all of the threads in a tile have reached the barrier, the average can be calculated.","pos":[10823,10913]},{"content":"Because the code executes for every thread, there is an <ph id=\"ph1\">`if`</ph> statement to only calculate the average on one thread.","pos":[10914,11029],"source":" Because the code executes for every thread, there is an `if` statement to only calculate the average on one thread."},{"content":"The average is stored in the averages variable.","pos":[11030,11077]},{"content":"The barrier is essentially the construct that controls calculations by tile, much as you might use a <ph id=\"ph1\">`for`</ph> loop.","pos":[11078,11190],"source":" The barrier is essentially the construct that controls calculations by tile, much as you might use a `for` loop."},{"content":"The data in the <ph id=\"ph1\">`averages`</ph> variable, because it is an <ph id=\"ph2\">`array`</ph> object, must be copied back to the host.","pos":[11200,11302],"source":"The data in the `averages` variable, because it is an `array` object, must be copied back to the host."},{"content":"This example uses the vector conversion operator.","pos":[11303,11352]},{"content":"In the complete example, you can change SAMPLESIZE to 4 and the code executes correctly without any other changes.","pos":[11362,11476]},{"content":"Race Conditions","pos":[14423,14438]},{"pos":[14442,14567],"content":"It might be tempting to create a <ph id=\"ph1\">`tile_static`</ph> variable named <ph id=\"ph2\">`total`</ph> and increment that variable for each thread, like this:","source":"It might be tempting to create a `tile_static` variable named `total` and increment that variable for each thread, like this:"},{"content":"The first problem with this approach is that <ph id=\"ph1\">`tile_static`</ph> variables cannot have initializers.","pos":[14769,14863],"source":"The first problem with this approach is that `tile_static` variables cannot have initializers."},{"content":"The second problem is that there is a race condition on the assignment to <ph id=\"ph1\">`total`</ph>, because all of the threads in the tile have access to the variable in no particular order.","pos":[14864,15037],"source":" The second problem is that there is a race condition on the assignment to `total`, because all of the threads in the tile have access to the variable in no particular order."},{"content":"You could program an algorithm to only allow one thread to access the total at each barrier, as shown next.","pos":[15038,15145]},{"content":"However, this solution is not extensible.","pos":[15146,15187]},{"content":"Memory Fences","pos":[15495,15508]},{"content":"There are two kinds of memory accesses that must be synchronized—global memory access and <ph id=\"ph1\">`tile_static`</ph> memory access.","pos":[15512,15630],"source":"There are two kinds of memory accesses that must be synchronized—global memory access and `tile_static` memory access."},{"content":"A <ph id=\"ph1\">`concurrency::array`</ph> object allocates only global memory.","pos":[15631,15690],"source":" A `concurrency::array` object allocates only global memory."},{"content":"A <ph id=\"ph1\">`concurrency::array_view`</ph> can reference global memory, <ph id=\"ph2\">`tile_static`</ph> memory, or both, depending on how it was constructed.","pos":[15691,15815],"source":" A `concurrency::array_view` can reference global memory, `tile_static` memory, or both, depending on how it was constructed."},{"content":"There are two kinds of memory that must be synchronized:","pos":[15817,15873]},{"content":"global memory","pos":[15883,15896]},{"content":"A <bpt id=\"p1\">*</bpt>memory fence<ept id=\"p1\">*</ept> ensures that memory accesses are available to other threads in the thread tile, and that memory accesses are executed according to program order.","pos":[15924,16086],"source":"A *memory fence* ensures that memory accesses are available to other threads in the thread tile, and that memory accesses are executed according to program order."},{"content":"To ensure this, compilers and processors do not reorder reads and writes across the fence.","pos":[16087,16177]},{"content":"In C++ AMP, a memory fence is created by a call to one of these methods:","pos":[16178,16250]},{"pos":[16259,16386],"content":"<bpt id=\"p1\">[</bpt>tile_barrier::wait Method<ept id=\"p1\">](reference/tile-barrier-class.md#wait)</ept>: Creates a fence around both global and <ph id=\"ph1\">`tile_static`</ph> memory.","source":"[tile_barrier::wait Method](reference/tile-barrier-class.md#wait): Creates a fence around both global and `tile_static` memory."},{"pos":[16394,16565],"content":"<bpt id=\"p1\">[</bpt>tile_barrier::wait_with_all_memory_fence Method<ept id=\"p1\">](reference/tile-barrier-class.md#wait_with_all_memory_fence)</ept>: Creates a fence around both global and <ph id=\"ph1\">`tile_static`</ph> memory.","source":"[tile_barrier::wait_with_all_memory_fence Method](reference/tile-barrier-class.md#wait_with_all_memory_fence): Creates a fence around both global and `tile_static` memory."},{"pos":[16573,16732],"content":"<bpt id=\"p1\">[</bpt>tile_barrier::wait_with_global_memory_fence Method<ept id=\"p1\">](reference/tile-barrier-class.md#wait_with_global_memory_fence)</ept>: Creates a fence around only global memory.","source":"[tile_barrier::wait_with_global_memory_fence Method](reference/tile-barrier-class.md#wait_with_global_memory_fence): Creates a fence around only global memory."},{"pos":[16740,16916],"content":"<bpt id=\"p1\">[</bpt>tile_barrier::wait_with_tile_static_memory_fence Method<ept id=\"p1\">](reference/tile-barrier-class.md#wait_with_tile_static_memory_fence)</ept>: Creates a fence around only <ph id=\"ph1\">`tile_static`</ph> memory.","source":"[tile_barrier::wait_with_tile_static_memory_fence Method](reference/tile-barrier-class.md#wait_with_tile_static_memory_fence): Creates a fence around only `tile_static` memory."},{"content":"Calling the specific fence that you require can improve the performance of your app.","pos":[16924,17008]},{"content":"The barrier type affects how the compiler and the hardware reorder statements.","pos":[17009,17087]},{"content":"For example, if you use a global memory fence, it applies only to global memory accesses and therefore, the compiler and the hardware might reorder reads and writes to <ph id=\"ph1\">`tile_static`</ph> variables on the two sides of the fence.","pos":[17088,17310],"source":" For example, if you use a global memory fence, it applies only to global memory accesses and therefore, the compiler and the hardware might reorder reads and writes to `tile_static` variables on the two sides of the fence."},{"content":"In the next example, the barrier synchronizes the writes to <ph id=\"ph1\">`tileValues`</ph>, a <ph id=\"ph2\">`tile_static`</ph> variable.","pos":[17317,17416],"source":"In the next example, the barrier synchronizes the writes to `tileValues`, a `tile_static` variable."},{"content":"In this example, <ph id=\"ph1\">`tile_barrier::wait_with_tile_static_memory_fence`</ph> is called instead of <ph id=\"ph2\">`tile_barrier::wait`</ph>.","pos":[17417,17527],"source":" In this example, `tile_barrier::wait_with_tile_static_memory_fence` is called instead of `tile_barrier::wait`."},{"content":"See Also","pos":[18549,18557]},{"content":"C++ AMP (C++ Accelerated Massive Parallelism)","pos":[18562,18607]},{"content":"tile_static Keyword","pos":[18681,18700]}],"content":"---\ntitle: \"Using Tiles | Microsoft Docs\"\nms.custom: \"\"\nms.date: \"11/04/2016\"\nms.reviewer: \"\"\nms.suite: \"\"\nms.technology: \n  - \"devlang-cpp\"\nms.tgt_pltfrm: \"\"\nms.topic: \"article\"\ndev_langs: \n  - \"C++\"\nms.assetid: acb86a86-2b7f-43f1-8fcf-bcc79b21d9a8\ncaps.latest.revision: 18\nauthor: \"mikeblome\"\nms.author: \"mblome\"\nmanager: \"ghogen\"\ntranslation.priority.ht: \n  - \"cs-cz\"\n  - \"de-de\"\n  - \"es-es\"\n  - \"fr-fr\"\n  - \"it-it\"\n  - \"ja-jp\"\n  - \"ko-kr\"\n  - \"pl-pl\"\n  - \"pt-br\"\n  - \"ru-ru\"\n  - \"tr-tr\"\n  - \"zh-cn\"\n  - \"zh-tw\"\n---\n# Using Tiles\nYou can use tiling to maximize the acceleration of your app. Tiling divides threads into equal rectangular subsets or *tiles*. If you use an appropriate tile size and tiled algorithm, you can get even more acceleration from your C++ AMP code. The basic components of tiling are:  \n  \n- `tile_static` variables. The primary benefit of tiling is the performance gain from `tile_static` access. Access to data in `tile_static` memory can be significantly faster than access to data in the global space (`array` or `array_view` objects). An instance of a `tile_static` variable is created for each tile, and all threads in the tile have access to the variable. In a typical tiled algorithm, data is copied into `tile_static` memory once from global memory and then accessed many times from the `tile_static` memory.  \n  \n- [tile_barrier::wait Method](reference/tile-barrier-class.md#wait). A call to `tile_barrier::wait` suspends execution of the current thread until all of the threads in the same tile reach the call to `tile_barrier::wait`. You cannot guarantee the order that the threads will run in, only that no threads in the tile will execute past the call to `tile_barrier::wait` until all of the threads have reached the call. This means that by using the `tile_barrier::wait` method, you can perform tasks on a tile-by-tile basis rather than a thread-by-thread basis. A typical tiling algorithm has code to initialize the `tile_static` memory for the whole tile followed by a call to `tile_barrer::wait`. Code that follows `tile_barrier::wait` contains computations that require access to all the `tile_static` values.  \n\n  \n-   Local and global indexing. You have access to the index of the thread relative to the entire `array_view` or `array` object and the index relative to the tile. Using the local index can make your code easier to read and debug. Typically, you use local indexing to access `tile_static` variables, and global indexing to access `array` and `array_view` variables.  \n  \n- [tiled_extent Class](../../parallel/amp/reference/tiled-extent-class.md) and [tiled_index Class](../../parallel/amp/reference/tiled-index-class.md). You use a `tiled_extent` object instead of an `extent` object in the `parallel_for_each` call. You use a `tiled_index` object instead of an `index` object in the `parallel_for_each` call.  \n  \n To take advantage of tiling, your algorithm must partition the compute domain into tiles and then copy the tile data into `tile_static` variables for faster access.  \n  \n## Example of Global, Tile, and Local Indices  \n The following diagram represents an 8x9 matrix of data that is arranged in 2x3 tiles.  \n  \n ![8&#45;by&#45;9 matrix divided into 2&#45;by&#45;3 tiles](../../parallel/amp/media/usingtilesmatrix.png \"usingtilesmatrix\")  \n  \n The following example displays the global, tile, and local indices of this tiled matrix. An `array_view` object is created by using elements of type `Description`. The `Description` holds the global, tile, and local indices of the element in the matrix. The code in the call to `parallel_for_each` sets the values of the global, tile, and local indices of each element. The output displays the values in the `Description` structures.  \n  \n```cpp  \n  \n#include <iostream>  \n#include <iomanip>  \n#include <Windows.h>  \n#include <amp.h>  \nusing namespace concurrency;  \n  \nconst int ROWS = 8;  \nconst int COLS = 9;  \n  \n// tileRow and tileColumn specify the tile that each thread is in.  \n// globalRow and globalColumn specify the location of the thread in the array_view.  \n// localRow and localColumn specify the location of the thread relative to the tile.  \nstruct Description {  \n    int value;  \n    int tileRow;  \n    int tileColumn;  \n    int globalRow;  \n    int globalColumn;  \n    int localRow;  \n    int localColumn;  \n};  \n  \n// A helper function for formatting the output.  \nvoid SetConsoleColor(int color) {  \n    int colorValue = (color == 0)  4 : 2;  \n    SetConsoleTextAttribute(GetStdHandle(STD_OUTPUT_HANDLE), colorValue);  \n}  \n  \n// A helper function for formatting the output.  \nvoid SetConsoleSize(int height, int width) {  \n    COORD coord; coord.X = width; coord.Y = height;  \n    SetConsoleScreenBufferSize(GetStdHandle(STD_OUTPUT_HANDLE), coord);  \n    SMALL_RECT* rect = new SMALL_RECT();  \n    rect->Left = 0; rect->Top = 0; rect->Right = width; rect->Bottom = height;  \n    SetConsoleWindowInfo(GetStdHandle(STD_OUTPUT_HANDLE), true, rect);  \n}  \n  \n// This method creates an 8x9 matrix of Description structures. In the call to parallel_for_each, the structure is updated   \n// with tile, global, and local indices.  \nvoid TilingDescription() {  \n    // Create 72 (8x9) Description structures.  \n    std::vector<Description> descs;  \n    for (int i = 0; i < ROWS * COLS; i++) {  \n        Description d = {i, 0, 0, 0, 0, 0, 0};  \n        descs.push_back(d);  \n    }  \n  \n    // Create an array_view from the Description structures.  \n    extent<2> matrix(ROWS, COLS);  \n    array_view<Description, 2> descriptions(matrix, descs);  \n  \n    // Update each Description with the tile, global, and local indices.  \n    parallel_for_each(descriptions.extent.tile< 2, 3>(),  \n [=] (tiled_index< 2, 3> t_idx) restrict(amp)   \n    {  \n        descriptions[t_idx].globalRow = t_idx.global[0];  \n        descriptions[t_idx].globalColumn = t_idx.global[1];  \n        descriptions[t_idx].tileRow = t_idx.tile[0];  \n        descriptions[t_idx].tileColumn = t_idx.tile[1];  \n        descriptions[t_idx].localRow = t_idx.local[0];  \n        descriptions[t_idx].localColumn= t_idx.local[1];  \n    });  \n  \n    // Print out the Description structure for each element in the matrix.  \n    // Tiles are displayed in red and green to distinguish them from each other.  \n    SetConsoleSize(100, 150);  \n    for (int row = 0; row < ROWS; row++) {  \n        for (int column = 0; column < COLS; column++) {  \n            SetConsoleColor((descriptions(row, column).tileRow + descriptions(row, column).tileColumn) % 2);  \n            std::cout << \"Value: \" << std::setw(2) << descriptions(row, column).value << \"      \";  \n        }  \n        std::cout << \"\\n\";  \n  \n        for (int column = 0; column < COLS; column++) {  \n            SetConsoleColor((descriptions(row, column).tileRow + descriptions(row, column).tileColumn) % 2);  \n            std::cout << \"Tile:   \" << \"(\" << descriptions(row, column).tileRow << \",\" << descriptions(row, column).tileColumn << \")  \";  \n        }  \n        std::cout << \"\\n\";  \n  \n        for (int column = 0; column < COLS; column++) {  \n            SetConsoleColor((descriptions(row, column).tileRow + descriptions(row, column).tileColumn) % 2);  \n            std::cout << \"Global: \" << \"(\" << descriptions(row, column).globalRow << \",\" << descriptions(row, column).globalColumn << \")  \";  \n        }  \n        std::cout << \"\\n\";  \n  \n        for (int column = 0; column < COLS; column++) {  \n            SetConsoleColor((descriptions(row, column).tileRow + descriptions(row, column).tileColumn) % 2);  \n            std::cout << \"Local:  \" << \"(\" << descriptions(row, column).localRow << \",\" << descriptions(row, column).localColumn << \")  \";  \n        }  \n        std::cout << \"\\n\";  \n        std::cout << \"\\n\";  \n    }  \n}  \n  \nvoid main() {  \n    TilingDescription();  \n    char wait;  \n    std::cin >> wait;  \n}  \n  \n```  \n  \n The main work of the example is in the definition of the `array_view` object and the call to `parallel_for_each`.  \n  \n1.  The vector of `Description` structures is copied into an 8x9 `array_view` object.  \n  \n2.  The `parallel_for_each` method is called with a `tiled_extent` object as the compute domain. The `tiled_extent` object is created by calling the `extent::tile()` method of the `descriptions` variable. The type parameters of the call to `extent::tile()`, `<2,3>`, specify that 2x3 tiles are created. Thus, the 8x9 matrix is tiled into 12 tiles, four rows and three columns.  \n  \n3.  The `parallel_for_each` method is called by using a `tiled_index<2,3>` object (`t_idx`) as the index. The type parameters of the index (`t_idx`) must match the type parameters of the compute domain (`descriptions.extent.tile< 2, 3>()`).  \n  \n4.  When each thread is executed, the index `t_idx` returns information about which tile the thread is in (`tiled_index::tile` property) and the location of the thread within the tile (`tiled_index::local` property).  \n  \n## Tile Synchronization—tile_static and tile_barrier::wait  \n The previous example illustrates the tile layout and indices, but is not in itself very useful.  Tiling becomes useful when the tiles are integral to the algorithm and exploit `tile_static` variables. Because all threads in a tile have access to `tile_static` variables, calls to `tile_barrier::wait` are used to synchronize access to the `tile_static` variables. Although all of the threads in a tile have access to the `tile_static` variables, there is no guaranteed order of execution of threads in the tile. The following example shows how to use `tile_static` variables and the `tile_barrier::wait` method to calculate the average value of each tile. Here are the keys to understanding the example:  \n  \n1.  The rawData is stored in an 8x8 matrix.  \n  \n2.  The tile size is 2x2. This creates a 4x4 grid of tiles and the averages can be stored in a 4x4 matrix by using an `array` object. There are only a limited number of types that you can capture by reference in an AMP-restricted function. The `array` class is one of them.  \n  \n3.  The matrix size and sample size are defined by using `#define` statements, because the type parameters to `array`, `array_view`, `extent`, and `tiled_index` must be constant values. You can also use `const int static` declarations. As an additional benefit, it is trivial to change the sample size to calculate the average over 4x4 tiles.  \n  \n4.  A `tile_static` 2x2 array of float values is declared for each tile. Although the declaration is in the code path for every thread, only one array is created for each tile in the matrix.  \n  \n5.  There is a line of code to copy the values in each tile to the `tile_static` array. For each thread, after the value is copied to the array, execution on the thread stops due to the call to `tile_barrier::wait`.  \n  \n6.  When all of the threads in a tile have reached the barrier, the average can be calculated. Because the code executes for every thread, there is an `if` statement to only calculate the average on one thread. The average is stored in the averages variable. The barrier is essentially the construct that controls calculations by tile, much as you might use a `for` loop.  \n  \n7.  The data in the `averages` variable, because it is an `array` object, must be copied back to the host. This example uses the vector conversion operator.  \n  \n8.  In the complete example, you can change SAMPLESIZE to 4 and the code executes correctly without any other changes.  \n  \n```cpp  \n  \n#include <iostream>  \n#include <amp.h>  \nusing namespace concurrency;  \n  \n#define SAMPLESIZE 2  \n#define MATRIXSIZE 8  \nvoid SamplingExample() {  \n  \n    // Create data and array_view for the matrix.  \n    std::vector<float> rawData;  \n    for (int i = 0; i < MATRIXSIZE * MATRIXSIZE; i++) {  \n        rawData.push_back((float)i);  \n    }  \n    extent<2> dataExtent(MATRIXSIZE, MATRIXSIZE);  \n    array_view<float, 2> matrix(dataExtent, rawData);  \n  \n    // Create the array for the averages.  \n    // There is one element in the output for each tile in the data.  \n    std::vector<float> outputData;  \n    int outputSize = MATRIXSIZE / SAMPLESIZE;  \n    for (int j = 0; j < outputSize * outputSize; j++) {  \n        outputData.push_back((float)0);  \n    }  \n    extent<2> outputExtent(MATRIXSIZE / SAMPLESIZE, MATRIXSIZE / SAMPLESIZE);  \n    array<float, 2> averages(outputExtent, outputData.begin(), outputData.end());  \n  \n    // Use tiles that are SAMPLESIZE x SAMPLESIZE.  \n    // Find the average of the values in each tile.  \n    // The only reference-type variable you can pass into the parallel_for_each call  \n    // is a concurrency::array.  \n    parallel_for_each(matrix.extent.tile<SAMPLESIZE, SAMPLESIZE>(),  \n [=, &averages] (tiled_index<SAMPLESIZE, SAMPLESIZE> t_idx) restrict(amp)   \n    {  \n        // Copy the values of the tile into a tile-sized array.  \n        tile_static float tileValues[SAMPLESIZE][SAMPLESIZE];  \n        tileValues[t_idx.local[0]][t_idx.local[1]] = matrix[t_idx];  \n  \n        // Wait for the tile-sized array to load before you calculate the average.  \n        t_idx.barrier.wait();  \n  \n        // If you remove the if statement, then the calculation executes for every  \n        // thread in the tile, and makes the same assignment to averages each time.  \n        if (t_idx.local[0] == 0 && t_idx.local[1] == 0) {  \n            for (int trow = 0; trow < SAMPLESIZE; trow++) {  \n                for (int tcol = 0; tcol < SAMPLESIZE; tcol++) {  \n                    averages(t_idx.tile[0],t_idx.tile[1]) += tileValues[trow][tcol];  \n                }  \n            }  \n            averages(t_idx.tile[0],t_idx.tile[1]) /= (float) (SAMPLESIZE * SAMPLESIZE);  \n        }  \n    });  \n  \n    // Print out the results.  \n    // You cannot access the values in averages directly. You must copy them  \n    // back to a CPU variable.  \n    outputData = averages;  \n    for (int row = 0; row < outputSize; row++) {  \n        for (int col = 0; col < outputSize; col++) {  \n            std::cout << outputData[row*outputSize + col] << \" \";  \n        }  \n        std::cout << \"\\n\";  \n    }  \n    // Output for SAMPLESSIZE = 2 is:  \n    //  4.5  6.5  8.5 10.5  \n    // 20.5 22.5 24.5 26.5  \n    // 36.5 38.5 40.5 42.5  \n    // 52.5 54.5 56.5 58.5  \n  \n    // Output for SAMPLESIZE = 4 is:  \n    // 13.5 17.5  \n    // 45.5 49.5  \n}  \n  \nint main() {  \n    SamplingExample();  \n}  \n  \n```  \n  \n## Race Conditions  \n It might be tempting to create a `tile_static` variable named `total` and increment that variable for each thread, like this:  \n  \n```cpp  \n \n// Do not do this.  \ntile_static float total;  \ntotal += matrix[t_idx];  \nt_idx.barrier.wait();\n\naverages(t_idx.tile[0],t_idx.tile[1]) /= (float) (SAMPLESIZE* SAMPLESIZE);\n\n \n```  \n  \n The first problem with this approach is that `tile_static` variables cannot have initializers. The second problem is that there is a race condition on the assignment to `total`, because all of the threads in the tile have access to the variable in no particular order. You could program an algorithm to only allow one thread to access the total at each barrier, as shown next. However, this solution is not extensible.  \n  \n```cpp  \n \n// Do not do this.  \ntile_static float total;  \nif (t_idx.local[0] == 0&& t_idx.local[1] == 0) {  \n    total = matrix[t_idx];  \n}  \nt_idx.barrier.wait();\n\n \nif (t_idx.local[0] == 0&& t_idx.local[1] == 1) {  \n    total += matrix[t_idx];  \n}  \nt_idx.barrier.wait();\n\n \n// etc.  \n \n```  \n  \n## Memory Fences  \n There are two kinds of memory accesses that must be synchronized—global memory access and `tile_static` memory access. A `concurrency::array` object allocates only global memory. A `concurrency::array_view` can reference global memory, `tile_static` memory, or both, depending on how it was constructed.  There are two kinds of memory that must be synchronized:  \n  \n-   global memory  \n  \n- `tile_static`  \n  \n A *memory fence* ensures that memory accesses are available to other threads in the thread tile, and that memory accesses are executed according to program order. To ensure this, compilers and processors do not reorder reads and writes across the fence. In C++ AMP, a memory fence is created by a call to one of these methods:  \n  \n\n- [tile_barrier::wait Method](reference/tile-barrier-class.md#wait): Creates a fence around both global and `tile_static` memory.  \n  \n- [tile_barrier::wait_with_all_memory_fence Method](reference/tile-barrier-class.md#wait_with_all_memory_fence): Creates a fence around both global and `tile_static` memory.  \n  \n- [tile_barrier::wait_with_global_memory_fence Method](reference/tile-barrier-class.md#wait_with_global_memory_fence): Creates a fence around only global memory.  \n  \n- [tile_barrier::wait_with_tile_static_memory_fence Method](reference/tile-barrier-class.md#wait_with_tile_static_memory_fence): Creates a fence around only `tile_static` memory.  \n\n  \n Calling the specific fence that you require can improve the performance of your app. The barrier type affects how the compiler and the hardware reorder statements. For example, if you use a global memory fence, it applies only to global memory accesses and therefore, the compiler and the hardware might reorder reads and writes to `tile_static` variables on the two sides of the fence.  \n  \n In the next example, the barrier synchronizes the writes to `tileValues`, a `tile_static` variable. In this example, `tile_barrier::wait_with_tile_static_memory_fence` is called instead of `tile_barrier::wait`.  \n  \n```cpp  \n \n// Using a tile_static memory fence.  \nparallel_for_each(matrix.extent.tile<SAMPLESIZE, SAMPLESIZE>(),  \n [=, &averages] (tiled_index<SAMPLESIZE, SAMPLESIZE> t_idx) restrict(amp)   \n{ *// Copy the values of the tile into a tile-sized array.  \n    tile_static float tileValues[SAMPLESIZE][SAMPLESIZE];  \n    tileValues[t_idx.local[0]][t_idx.local[1]] = matrix[t_idx];  \n *// Wait for the tile-sized array to load before calculating the average.  \n    t_idx.barrier.wait_with_tile_static_memory_fence();\n\n *// If you remove the if statement, then the calculation executes for every *// thread in the tile, and makes the same assignment to averages each time.  \n    if (t_idx.local[0] == 0&& t_idx.local[1] == 0) {  \n    for (int trow = 0; trow <SAMPLESIZE; trow++) {  \n    for (int tcol = 0; tcol <SAMPLESIZE; tcol++) {  \n    averages(t_idx.tile[0],t_idx.tile[1]) += tileValues[trow][tcol];  \n }  \n }  \n    averages(t_idx.tile[0],t_idx.tile[1]) /= (float) (SAMPLESIZE* SAMPLESIZE);\n\n }  \n});\n\n \n```  \n  \n## See Also  \n [C++ AMP (C++ Accelerated Massive Parallelism)](../../parallel/amp/cpp-amp-cpp-accelerated-massive-parallelism.md)   \n [tile_static Keyword](../../cpp/tile-static-keyword.md)\n\n"}