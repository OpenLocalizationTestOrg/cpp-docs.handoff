<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="markdown" source-language="en-US" target-language="de-de">
    <header>
      <tool tool-id="mdxliff" tool-name="mdxliff" tool-version="1.0-787e512" tool-company="Microsoft" />
      <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">30059308dac121ce1cbfef97d50636cf69d8f6b1</xliffext:olfilehash>
      <xliffext:olfilepath xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">docs\parallel\amp\using-tiles.md</xliffext:olfilepath>
      <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">ht</xliffext:oltranslationpriority>
      <xliffext:oltranslationtype xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">Human Translation</xliffext:oltranslationtype>
      <xliffext:olskeletonhash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">d9fe4d3b269fa5383cace5e3782ae29651b5cef2</xliffext:olskeletonhash>
      <xliffext:olxliffhash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">d58ff62fe3092781e879dd3c429e758430a46a31</xliffext:olxliffhash>
    </header>
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Using Tiles | Microsoft Docs</source>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Using Tiles</source>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>You can use tiling to maximize the acceleration of your app.</source>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Tiling divides threads into equal rectangular subsets or <bpt id="p1">*</bpt>tiles<ept id="p1">*</ept>.</source>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>If you use an appropriate tile size and tiled algorithm, you can get even more acceleration from your C++ AMP code.</source>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>The basic components of tiling are:</source>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>variables.</source>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>The primary benefit of tiling is the performance gain from <ph id="ph1">`tile_static`</ph> access.</source>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>Access to data in <ph id="ph1">`tile_static`</ph> memory can be significantly faster than access to data in the global space (<ph id="ph2">`array`</ph> or <ph id="ph3">`array_view`</ph> objects).</source>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>An instance of a <ph id="ph1">`tile_static`</ph> variable is created for each tile, and all threads in the tile have access to the variable.</source>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>In a typical tiled algorithm, data is copied into <ph id="ph1">`tile_static`</ph> memory once from global memory and then accessed many times from the <ph id="ph2">`tile_static`</ph> memory.</source>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>tile_barrier::wait Method<ept id="p1">](reference/tile-barrier-class.md#wait)</ept>.</source>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>A call to <ph id="ph1">`tile_barrier::wait`</ph> suspends execution of the current thread until all of the threads in the same tile reach the call to <ph id="ph2">`tile_barrier::wait`</ph>.</source>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>You cannot guarantee the order that the threads will run in, only that no threads in the tile will execute past the call to <ph id="ph1">`tile_barrier::wait`</ph> until all of the threads have reached the call.</source>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>This means that by using the <ph id="ph1">`tile_barrier::wait`</ph> method, you can perform tasks on a tile-by-tile basis rather than a thread-by-thread basis.</source>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>A typical tiling algorithm has code to initialize the <ph id="ph1">`tile_static`</ph> memory for the whole tile followed by a call to <ph id="ph2">`tile_barrer::wait`</ph>.</source>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Code that follows <ph id="ph1">`tile_barrier::wait`</ph> contains computations that require access to all the <ph id="ph2">`tile_static`</ph> values.</source>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>Local and global indexing.</source>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>You have access to the index of the thread relative to the entire <ph id="ph1">`array_view`</ph> or <ph id="ph2">`array`</ph> object and the index relative to the tile.</source>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>Using the local index can make your code easier to read and debug.</source>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Typically, you use local indexing to access <ph id="ph1">`tile_static`</ph> variables, and global indexing to access <ph id="ph2">`array`</ph> and <ph id="ph3">`array_view`</ph> variables.</source>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>tiled_extent Class<ept id="p1">](../../parallel/amp/reference/tiled-extent-class.md)</ept> and <bpt id="p2">[</bpt>tiled_index Class<ept id="p2">](../../parallel/amp/reference/tiled-index-class.md)</ept>.</source>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>You use a <ph id="ph1">`tiled_extent`</ph> object instead of an <ph id="ph2">`extent`</ph> object in the <ph id="ph3">`parallel_for_each`</ph> call.</source>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>You use a <ph id="ph1">`tiled_index`</ph> object instead of an <ph id="ph2">`index`</ph> object in the <ph id="ph3">`parallel_for_each`</ph> call.</source>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>To take advantage of tiling, your algorithm must partition the compute domain into tiles and then copy the tile data into <ph id="ph1">`tile_static`</ph> variables for faster access.</source>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>Example of Global, Tile, and Local Indices</source>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>The following diagram represents an 8x9 matrix of data that is arranged in 2x3 tiles.</source>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>8&amp;#45;by&amp;#45;9 matrix divided into 2&amp;#45;by&amp;#45;3 tiles</source>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>The following example displays the global, tile, and local indices of this tiled matrix.</source>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>An <ph id="ph1">`array_view`</ph> object is created by using elements of type <ph id="ph2">`Description`</ph>.</source>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>The <ph id="ph1">`Description`</ph> holds the global, tile, and local indices of the element in the matrix.</source>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>The code in the call to <ph id="ph1">`parallel_for_each`</ph> sets the values of the global, tile, and local indices of each element.</source>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>The output displays the values in the <ph id="ph1">`Description`</ph> structures.</source>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>The main work of the example is in the definition of the <ph id="ph1">`array_view`</ph> object and the call to <ph id="ph2">`parallel_for_each`</ph>.</source>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>The vector of <ph id="ph1">`Description`</ph> structures is copied into an 8x9 <ph id="ph2">`array_view`</ph> object.</source>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>The <ph id="ph1">`parallel_for_each`</ph> method is called with a <ph id="ph2">`tiled_extent`</ph> object as the compute domain.</source>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>The <ph id="ph1">`tiled_extent`</ph> object is created by calling the <ph id="ph2">`extent::tile()`</ph> method of the <ph id="ph3">`descriptions`</ph> variable.</source>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>The type parameters of the call to <ph id="ph1">`extent::tile()`</ph>, <ph id="ph2">`&lt;2,3&gt;`</ph>, specify that 2x3 tiles are created.</source>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>Thus, the 8x9 matrix is tiled into 12 tiles, four rows and three columns.</source>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>The <ph id="ph1">`parallel_for_each`</ph> method is called by using a <ph id="ph2">`tiled_index&lt;2,3&gt;`</ph> object (<ph id="ph3">`t_idx`</ph>) as the index.</source>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>The type parameters of the index (<ph id="ph1">`t_idx`</ph>) must match the type parameters of the compute domain (<ph id="ph2">`descriptions.extent.tile&lt; 2, 3&gt;()`</ph>).</source>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>When each thread is executed, the index <ph id="ph1">`t_idx`</ph> returns information about which tile the thread is in (<ph id="ph2">`tiled_index::tile`</ph> property) and the location of the thread within the tile (<ph id="ph3">`tiled_index::local`</ph> property).</source>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>Tile Synchronization—tile_static and tile_barrier::wait</source>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>The previous example illustrates the tile layout and indices, but is not in itself very useful.</source>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Tiling becomes useful when the tiles are integral to the algorithm and exploit <ph id="ph1">`tile_static`</ph> variables.</source>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>Because all threads in a tile have access to <ph id="ph1">`tile_static`</ph> variables, calls to <ph id="ph2">`tile_barrier::wait`</ph> are used to synchronize access to the <ph id="ph3">`tile_static`</ph> variables.</source>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>Although all of the threads in a tile have access to the <ph id="ph1">`tile_static`</ph> variables, there is no guaranteed order of execution of threads in the tile.</source>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>The following example shows how to use <ph id="ph1">`tile_static`</ph> variables and the <ph id="ph2">`tile_barrier::wait`</ph> method to calculate the average value of each tile.</source>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>Here are the keys to understanding the example:</source>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>The rawData is stored in an 8x8 matrix.</source>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>The tile size is 2x2.</source>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>This creates a 4x4 grid of tiles and the averages can be stored in a 4x4 matrix by using an <ph id="ph1">`array`</ph> object.</source>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>There are only a limited number of types that you can capture by reference in an AMP-restricted function.</source>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>The <ph id="ph1">`array`</ph> class is one of them.</source>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>The matrix size and sample size are defined by using <ph id="ph1">`#define`</ph> statements, because the type parameters to <ph id="ph2">`array`</ph>, <ph id="ph3">`array_view`</ph>, <ph id="ph4">`extent`</ph>, and <ph id="ph5">`tiled_index`</ph> must be constant values.</source>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>You can also use <ph id="ph1">`const int static`</ph> declarations.</source>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>As an additional benefit, it is trivial to change the sample size to calculate the average over 4x4 tiles.</source>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>A <ph id="ph1">`tile_static`</ph> 2x2 array of float values is declared for each tile.</source>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>Although the declaration is in the code path for every thread, only one array is created for each tile in the matrix.</source>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>There is a line of code to copy the values in each tile to the <ph id="ph1">`tile_static`</ph> array.</source>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>For each thread, after the value is copied to the array, execution on the thread stops due to the call to <ph id="ph1">`tile_barrier::wait`</ph>.</source>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>When all of the threads in a tile have reached the barrier, the average can be calculated.</source>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>Because the code executes for every thread, there is an <ph id="ph1">`if`</ph> statement to only calculate the average on one thread.</source>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>The average is stored in the averages variable.</source>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>The barrier is essentially the construct that controls calculations by tile, much as you might use a <ph id="ph1">`for`</ph> loop.</source>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>The data in the <ph id="ph1">`averages`</ph> variable, because it is an <ph id="ph2">`array`</ph> object, must be copied back to the host.</source>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>This example uses the vector conversion operator.</source>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>In the complete example, you can change SAMPLESIZE to 4 and the code executes correctly without any other changes.</source>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>Race Conditions</source>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>It might be tempting to create a <ph id="ph1">`tile_static`</ph> variable named <ph id="ph2">`total`</ph> and increment that variable for each thread, like this:</source>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>The first problem with this approach is that <ph id="ph1">`tile_static`</ph> variables cannot have initializers.</source>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>The second problem is that there is a race condition on the assignment to <ph id="ph1">`total`</ph>, because all of the threads in the tile have access to the variable in no particular order.</source>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>You could program an algorithm to only allow one thread to access the total at each barrier, as shown next.</source>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>However, this solution is not extensible.</source>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>Memory Fences</source>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>There are two kinds of memory accesses that must be synchronized—global memory access and <ph id="ph1">`tile_static`</ph> memory access.</source>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>A <ph id="ph1">`concurrency::array`</ph> object allocates only global memory.</source>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>A <ph id="ph1">`concurrency::array_view`</ph> can reference global memory, <ph id="ph2">`tile_static`</ph> memory, or both, depending on how it was constructed.</source>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>There are two kinds of memory that must be synchronized:</source>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>global memory</source>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>A <bpt id="p1">*</bpt>memory fence<ept id="p1">*</ept> ensures that memory accesses are available to other threads in the thread tile, and that memory accesses are executed according to program order.</source>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>To ensure this, compilers and processors do not reorder reads and writes across the fence.</source>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>In C++ AMP, a memory fence is created by a call to one of these methods:</source>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>tile_barrier::wait Method<ept id="p1">](reference/tile-barrier-class.md#wait)</ept>: Creates a fence around both global and <ph id="ph1">`tile_static`</ph> memory.</source>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>tile_barrier::wait_with_all_memory_fence Method<ept id="p1">](reference/tile-barrier-class.md#wait_with_all_memory_fence)</ept>: Creates a fence around both global and <ph id="ph1">`tile_static`</ph> memory.</source>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>tile_barrier::wait_with_global_memory_fence Method<ept id="p1">](reference/tile-barrier-class.md#wait_with_global_memory_fence)</ept>: Creates a fence around only global memory.</source>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>tile_barrier::wait_with_tile_static_memory_fence Method<ept id="p1">](reference/tile-barrier-class.md#wait_with_tile_static_memory_fence)</ept>: Creates a fence around only <ph id="ph1">`tile_static`</ph> memory.</source>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>Calling the specific fence that you require can improve the performance of your app.</source>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>The barrier type affects how the compiler and the hardware reorder statements.</source>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>For example, if you use a global memory fence, it applies only to global memory accesses and therefore, the compiler and the hardware might reorder reads and writes to <ph id="ph1">`tile_static`</ph> variables on the two sides of the fence.</source>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>In the next example, the barrier synchronizes the writes to <ph id="ph1">`tileValues`</ph>, a <ph id="ph2">`tile_static`</ph> variable.</source>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>In this example, <ph id="ph1">`tile_barrier::wait_with_tile_static_memory_fence`</ph> is called instead of <ph id="ph2">`tile_barrier::wait`</ph>.</source>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>See Also</source>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>C++ AMP (C++ Accelerated Massive Parallelism)</source>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>tile_static Keyword</source>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>